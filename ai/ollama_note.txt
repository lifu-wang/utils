
#-------------------
# NVIDIA 

$ ubuntu-drivers list
$ apt install -y nvidia-driver-580-server nvidia-cuda-toolkit
$ reboot

$ nvidia-smi
|   0  NVIDIA GeForce RTX 4060 ...    Off |   ... 

$ nvcc --version
Cuda compilation tools, release 12.0, V12.0.140

$ gpustat
alien18-R2                             Mon Nov 24 15:50:15 2025  580.95.05
[0] NVIDIA GeForce RTX 4060 Laptop GPU | 50Â°C,  16 % |    14 /  8188 MB | gdm(4M)



#-------------------
# mcp server by uvx 

PS C:\Users\miswl> powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
C:\Users\miswl>uvx pycowsay "hello world"
  -----------
< hello world >
  -----------
   \   ^__^
    \  (oo)\_______
       (__)\       )\/\
           ||----w |
           ||     ||

$ curl -LsSf https://astral.sh/uv/install.sh | sh
$ uvx pycowsay "hello world"

$ mkdir ~/proj/workspace/mcp_weather 
$ cd ~/proj/workspace/mcp_weather

# from https://github.com/daycyberwox/mcp-weather-app/blob/main/weather.py
$ cat > weather.py

# VScode/Cline MCP servers: Configure -> Configure MCP servers -> save/enable
{
  "mcpServers": {
    "weather": {
      "command": "/home/lifu/.local/bin/uv",
      "args": [
        "--directory",
        "/home/lifu/proj/workspace/mcp_weather",
        "run",
        "weather.py"
      ]
    }
  }
}

# Make sure uv run without errors
$ apt install python3-httpx
$ PIP_BREAK_SYSTEM_PACKAGES=1 pip install mcp fastmcp httpx

$ uv --directory /home/lifu/proj/workspace/mcp_weather run weather.py

# Go VScode/Cline: ask "what's my weather"

#--------
# Spark
#
# - No need to change config timeout
# - use qwen3-coder

#-------------------------
# Ollama install, config & service

$ curl -fsSL https://ollama.com/install.sh | sh
$ ollama list; ollama run gpt-oss --verbose; ollama show gpt-oss

C:\Users\miswl\.ollama>more C:\Users\miswl\.ollama\config.yaml
server:
  timeout: 300000        # in milliseconds, i.e., 5 minutes
  max_context_tokens: 8192
logging:
  level: info

# Ctrl-Alt-Del: kill all ollama process

# ollama start: start background servr
C:\> ollama start
C:\> ollama run llama3.2:latest "hello"

# ollama serve: expose HTTP/REST API server, default is port 11434
C:\> ollama serve llama3.2:latest --port 11434
C:\> curl -X POST http://localhost:11434/predict -d '{"prompt":"Hello"}'
C:\> ollama stop llama3.2:latest

# check port
C:\> netstat -ano | findstr 11434

lifu@PCHOME:~$ sudo lsof -i :11434
lifu@PCHOME:~$ sudo netstat -tuln | grep 11434


#--------------------
# Window: cline error

# Errors: {"message":"Ollama request timed out after 30 seconds","modelId":"qwen3-coder:latest","providerId":"ollama"}
# Reason: 
#   C:\> ollama run qwen3-coder:latest "hi" --> if taking 30+ sec, model is too big
#   C:\> ollama serve
# 

# Fix:
# VS code: Ctrl+Shift+p: Open user settings (JSON)
# add between {}
      "cline.timeoutMs": 120000
      
#------------------------
# Mac OS: Ollama install

# clean
sudo rm -rf /Applications/Ollama.app
sudo rm /usr/local/bin/ollama
rm -rf "~/Library/Application Support/Ollama"
rm -rf "~/Library/Saved Application State/com.electron.ollama.savedState"
rm -rf ~/Library/Caches/com.electron.ollama/
rm -rf ~/Library/Caches/ollama
rm -rf ~/Library/WebKit/com.electron.ollama
rm -rf ~/.ollama

# brew install
brew install ollama

# brew services start ollama
brew services start ollama

